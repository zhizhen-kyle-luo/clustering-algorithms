{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Quality of Data Outputed\n",
    "#By: Ashley You"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attempt at using disamby package to disambiguate\n",
    "#By: Ashley You\n",
    "#last updated: 4/4/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openalexapi\n",
    "import requests, json\n",
    "import pandas as pd\n",
    "import ast\n",
    "import math\n",
    "import heapq\n",
    "import csv\n",
    "import os\n",
    "\n",
    "name_search = [\"william pao\",\n",
    "    \"frederick suchy\",\n",
    "    \"malcolm cox\",\n",
    "    \"nancy cooke\",\n",
    "    \"allan sniderman\",\n",
    "    \"vincent dennis\",\n",
    "    \"alessandra pernis\",\n",
    "    \"john minna\",\n",
    "    \"dennis bier\",\n",
    "    \"roger pomerantz\"]\n",
    "\n",
    "\n",
    "#Getting all author ids through search of first and last name:\n",
    "base_url = 'https://api.openalex.org/'\n",
    "def get_authorIDs(name):\n",
    "    listofIDs = []\n",
    "    page = 1\n",
    "    full_query= f'https://api.openalex.org/authors?search={name}&page={page}'\n",
    "    response = requests.get(full_query)\n",
    "    visualize_data = response.json()\n",
    "    num_pages = math.ceil(visualize_data['meta']['count']/25)\n",
    "    \n",
    "    while page <= num_pages:\n",
    "        full_query= f'https://api.openalex.org/authors?search={name}&page={page}'\n",
    "        response = requests.get(full_query)\n",
    "        visualize_data = response.json()\n",
    "        for result in visualize_data['results']:\n",
    "            openalex_id = result['id'].replace(\"https://openalex.org/\", \"\")\n",
    "            listofIDs.append(openalex_id)\n",
    "            \n",
    "            #for concepts in result['x_concepts']:\n",
    "                #if concepts['display_name'] == 'Medicine':\n",
    "        page += 1 \n",
    "\n",
    "    print(f'There are {len(listofIDs)} author ids for {name}')\n",
    "    return listofIDs\n",
    "\n",
    "#get_authorIDs(\"William pao\")\n",
    "\n",
    "\n",
    "#Finds all work_ids with given authorId\n",
    "def work_id(givenAuthorID):\n",
    "        page = 'page={}'\n",
    "        filtered_works_url = f'https://api.openalex.org/works?filter=author.id:{givenAuthorID}&{page}'\n",
    "        page = 1\n",
    "        has_more_pages = True\n",
    "        fewer_than_10000_results = True\n",
    "        all_worksID = []\n",
    "\n",
    "        # loop through pages\n",
    "        while has_more_pages and fewer_than_10000_results:\n",
    "\n",
    "            # set page value and request page from OpenAlex\n",
    "            url = filtered_works_url.format(page)\n",
    "            page_with_results = requests.get(url).json()\n",
    "\n",
    "            # loop through partial list of results\n",
    "            results = page_with_results['results']\n",
    "            for i,work in enumerate(results):\n",
    "                openalex_id = work['id'].replace(\"https://openalex.org/\", \"\")\n",
    "                all_worksID.append(openalex_id)\n",
    "            # next page\n",
    "            page += 1\n",
    "\n",
    "            # end loop when either there are no more results on the requested page \n",
    "            # or the next request would exceed 15 results\n",
    "            per_page = page_with_results['meta']['per_page']\n",
    "            has_more_pages = len(results) == per_page\n",
    "            fewer_than_10000_results = per_page * page <= 10000\n",
    "        print(f'There are {len(all_worksID)} works for {givenAuthorID}')\n",
    "        return (all_worksID)\n",
    "#work_id('A2250212419')\n",
    "\n",
    "#Tests if finding asci names and concepts works under a small scale\n",
    "#----------------------TESTER----------------- FOR THE findAAConcepts FUNCTION AFTER\n",
    "def findIndvConcepts(names):\n",
    "    authorsConcepts = {}\n",
    "    dir = os.path.dirname(os.path.realpath(\"Open_AlexMerging.ipynb\")).replace(\"open_alex_data\", \"asci_aap_data\")\n",
    "    os.chdir(dir)\n",
    "    with open(r\"asci_aap_dataJSONUpdated.json\") as fileJson:        \n",
    "        data = json.load(fileJson)\n",
    "        allData = data[\"people\"]\n",
    "\n",
    "    for name in names:\n",
    "        for indv in allData:\n",
    "            first = indv[\"first_name\"].lower()\n",
    "            last = indv[\"last_name\"].lower()\n",
    "            if (first+\" \"+last)== name.lower():                                               \n",
    "                authorsConcepts[name] = (ast.literal_eval(indv[\"original specialization\"]))\n",
    "            \n",
    "        if (name in authorsConcepts):\n",
    "            print(f'{name} successfully found')\n",
    "        else:\n",
    "            authorsConcepts[name] = []\n",
    "            print(f'{name} not found') \n",
    "\n",
    "    return authorsConcepts\n",
    "#findIndvConcepts([\"William Pao\",\n",
    "                  #\"Ashley You\",\n",
    "                  #\"Kjersti Aagaard\"])\n",
    "                  #\"E. Abel\",\n",
    "                  #\"Janis Abkowitz\"])\n",
    "\n",
    "\n",
    "#Goes through asci/aap data and gets name and concepts\n",
    "def findAAConcepts():\n",
    "    authorsConcepts = {}\n",
    "    dir = os.path.dirname(os.path.realpath(\"Open_AlexMerging.ipynb\")).replace(\"open_alex_data\", \"asci_aap_data\")\n",
    "    os.chdir(dir)\n",
    "    with open(r\"asci_aap_dataJSONUpdated.json\") as fileJson:        \n",
    "        data = json.load(fileJson)\n",
    "        allData = data[\"people\"]\n",
    "        print(f'There are {len(allData)} amount of people in ASCI/AAP json file')\n",
    "\n",
    "    for indv in allData:\n",
    "        first = indv[\"first_name\"].lower()\n",
    "        last = indv[\"last_name\"].lower()\n",
    "        name = first+\" \"+last\n",
    "        if len(indv[\"original specialization\"])!= 2:\n",
    "            authorsConcepts[name] = (ast.literal_eval(indv[\"original specialization\"]))\n",
    "    #new_dict = {key: value for key, value in authorsConcepts.items() if value}\n",
    "    print(f'There are {len(authorsConcepts)} amount of people with specialites listed in ASCI/AAP json file')\n",
    "    return authorsConcepts  \n",
    "#findAAConcepts()   \n",
    "\n",
    "def findAANames():\n",
    "    authors = []\n",
    "    dir = os.path.dirname(os.path.realpath(\"Open_AlexMerging.ipynb\")).replace(\"open_alex_data\", \"asci_aap_data\")\n",
    "    os.chdir(dir)\n",
    "    with open(r\"asci_aap_dataJSONUpdated.json\") as fileJson:        \n",
    "        data = json.load(fileJson)\n",
    "        allData = data[\"people\"]\n",
    "        print(f'There are {len(allData)} amount of people in ASCI/AAP json file')\n",
    "\n",
    "    for indv in allData:\n",
    "        first = indv[\"first_name\"].lower()\n",
    "        last = indv[\"last_name\"].lower()\n",
    "        name = first+\" \"+last\n",
    "        authors.append(name)\n",
    "    return authors  \n",
    "#findAANames()\n",
    "\n",
    "#finished\n",
    "#finds author concepts given list of names\n",
    "#filter through medicine \n",
    "def authorConcepts(people):\n",
    "    authors = {}\n",
    "    for name in people:\n",
    "        totalConcepts = []\n",
    "        authorIds = get_authorIDs(name)\n",
    "        for id in authorIds:\n",
    "            authorTopics= {}\n",
    "            tempConcepts = []\n",
    "            full_query= f'https://api.openalex.org/authors/{id}'\n",
    "            response = requests.get(full_query)\n",
    "            visualize_data = response.json()\n",
    "            for concepts in visualize_data[\"x_concepts\"]:\n",
    "                if (float(concepts['score']) >= 90.0 and float(concepts['level']) >= 1) or concepts['display_name']== \"Medicine\":\n",
    "                    tempConcepts.append(concepts['display_name'])\n",
    "            authorTopics[id]= tempConcepts\n",
    "            totalConcepts.append(authorTopics)\n",
    "        authors[name]= totalConcepts\n",
    "    return authors \n",
    "#authorConcepts(['Kjersti Aagaard'])\n",
    "\n",
    "\n",
    "#finds all work details given work id\n",
    "def findWork(workId):\n",
    "    fullquery = base_url+'works/'+workId\n",
    "    response = requests.get(fullquery)\n",
    "    visualize_data = response.json()\n",
    "    visualize_data.pop(\"abstract_inverted_index\")\n",
    "    visualize_data.pop(\"related_works\")\n",
    "    visualize_data.pop(\"ngrams_url\")\n",
    "    #clean the unicode\n",
    "    #visualize_data[\"\"]\n",
    "    return visualize_data\n",
    "#findWork('W2139236349')\n",
    "\n",
    "\n",
    "#finds work concepts given work link\n",
    "def workConcepts(workId):\n",
    "    totalDict = {}\n",
    "    totalWorkConcepts = []\n",
    "    allinfo = findWork(workId)\n",
    "    for concept in allinfo['concepts']:\n",
    "        if float(concept['score']) >= 0.3 and float(concept['level']) >= 2 or concept['display_name']== \"Medicine\" :\n",
    "            totalWorkConcepts.append(concept['display_name'])\n",
    "    totalDict[workId] = totalWorkConcepts\n",
    "\n",
    "    return totalDict\n",
    "#workConcepts(\"W2005052157\")\n",
    "\n",
    "\n",
    "#checks which concepts occur the most often in a work\n",
    "#def checkConcepts(conceptlist):\n",
    "    count_dict = {}\n",
    "    temp_dict = {}\n",
    "    for element in conceptlist:\n",
    "        if element in count_dict:\n",
    "            count_dict[element] += 1\n",
    "        else:\n",
    "            count_dict[element] = 1\n",
    "\n",
    "    for element, count in count_dict.items():\n",
    "        if element!='Medicine':\n",
    "            temp_dict[element] = count\n",
    "    # Find the three largest values\n",
    "    largest_values = heapq.nlargest(3, temp_dict.values())\n",
    "\n",
    "    # Find the keys corresponding to the largest values\n",
    "    largest_keys = []\n",
    "    for key, value in temp_dict.items():\n",
    "        if value in largest_values:\n",
    "            largest_keys.append(key)\n",
    "\n",
    "    # Print the largest values and their keys\n",
    "    final_dict = {}\n",
    "    for i in range(len(largest_values)):\n",
    "        #print(\"{}. {} has a value of {}\".format(i+1, largest_keys[i], largest_values[i]))\n",
    "        final_dict[largest_keys[i]] = largest_values[i]\n",
    "        \n",
    "    return final_dict\n",
    "\n",
    "def checkConcepts(conceptlist):\n",
    "    count_dict = {}\n",
    "    for element in conceptlist:\n",
    "        if element!='Medicine':\n",
    "            if element in count_dict:\n",
    "                count_dict[element] += 1\n",
    "            else:\n",
    "                count_dict[element] = 1\n",
    "\n",
    "    # Find the three largest values\n",
    "    largest_values = heapq.nlargest(3, count_dict.values())\n",
    "\n",
    "    # Find the keys corresponding to the largest values, stores first 3\n",
    "    final_dict = {}\n",
    "    n = 0\n",
    "    for key, value in count_dict.items():\n",
    "        if n < 3:\n",
    "            if value in largest_values:\n",
    "                final_dict[key] = value\n",
    "                n += 1\n",
    "        else: \n",
    "             break\n",
    "    return final_dict\n",
    "\n",
    "testList= ['Medicine',\n",
    "  'Medicine', \n",
    "  'Medicine',\n",
    "  'Eosinophilic esophagitis',\n",
    "  'Budesonide',\n",
    "  'Internal medicine',\n",
    "  'Heartburn',\n",
    "  'Eosinophilia',\n",
    "  'Gastroenterology',\n",
    "  'Nausea',\n",
    "  'Vomiting',\n",
    "  'Corticosteroid',\n",
    "  'Adverse effect',\n",
    "  'Esophagitis',\n",
    "  'Eosinophilic esophagitis',\n",
    "  'Internal Medicine',\n",
    "  'Budesonide']\n",
    "#checkConcepts(testList)\n",
    "\n",
    "def findWorkConcepts(names): #keys(1. name, 2. authorId 3. workId)\n",
    "    #searches to get author ids\n",
    "    finalDict = {}\n",
    "    count = 0\n",
    "    for name in names:\n",
    "        authorIDs = get_authorIDs(name)\n",
    "        listofAuthors = []\n",
    "        for id in authorIDs:\n",
    "            tempDict = {}\n",
    "            tempWorkList = []\n",
    "            workIds = work_id(id)\n",
    "            for wID in workIds:\n",
    "               if 'Medicine' in workConcepts(wID)[wID]: #preliminary filter\n",
    "                tempWorkList.append(workConcepts(wID))\n",
    "               else: \n",
    "                   count +=1\n",
    "            tempDict[id] = tempWorkList\n",
    "            listofAuthors.append(tempDict)\n",
    "        finalDict[name] = listofAuthors\n",
    "    print(f'{count} amount of workIds did not have Medicine in their concepts')\n",
    "    return finalDict\n",
    "                \n",
    "    #searches to get work ids\n",
    "    #access work ids\n",
    "    #access concepts in work id\n",
    "    #loops through concepts in work id and saves it \n",
    "#findWorkConcepts([\"Seema Aceves\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanFinal(names): \n",
    "    finalDict = {}\n",
    "    count = 0\n",
    "    inputDict = findWorkConcepts(names)  \n",
    "    for name in names:\n",
    "        merged_dict = {}\n",
    "        combined_listConcepts = []\n",
    "        for alist in inputDict[name]: #list of author id dictionaries\n",
    "            for aID in alist: #list of author ids\n",
    "                for dictionary in alist[aID]:\n",
    "                    merged_dict.update(dictionary)\n",
    "        for value in merged_dict.values():\n",
    "        # Combine all lists into one\n",
    "            combined_listConcepts += value\n",
    "        mostCommonConceptsdict = checkConcepts(combined_listConcepts)\n",
    "        concept_list = list(mostCommonConceptsdict.keys()) #list of top 5 most common concepts for person\n",
    "        for key in list(merged_dict.keys()):\n",
    "            checker = False\n",
    "            for concept in concept_list:\n",
    "                if concept in merged_dict[key]:\n",
    "                    checker = True\n",
    "                else:\n",
    "                    pass\n",
    "            if checker == False:\n",
    "                del merged_dict[key] \n",
    "                count +=1\n",
    "        finalDict[name] = merged_dict\n",
    "    print(f'{count} amount of workIds did not have any of the top 3 most common author concepts in their work concepts')\n",
    "    return finalDict\n",
    "\n",
    "cleanFinal(name_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------WRITES FUNCTION ABOVE TO A CSV FILE WITH FILTERING RESULTS----------------\n",
    "def cleanFinalOverview(names,title):\n",
    "    try:\n",
    "        with open(title, mode='w', newline ='') as file:\n",
    "            #print(os.getcwd())\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"name\", \"starting_works\", \"key_concepts\", \"result_works\", \"works_removed\"]) \n",
    "            finalDict = {}\n",
    "            count = 0\n",
    "            lenPeople = len(names)\n",
    "            print(f'Going through {lenPeople} amount of people.........')\n",
    "            inputDict = findWorkConcepts(names)\n",
    "            for name in names:\n",
    "                merged_dict = {}\n",
    "                combined_listConcepts = []\n",
    "                for alist in inputDict[name]: #list of author id dictionaries\n",
    "                    for aID in alist: #list of author ids\n",
    "                        for dictionary in alist[aID]:\n",
    "                            merged_dict.update(dictionary)\n",
    "                for value in merged_dict.values():\n",
    "                # Combine all lists into one\n",
    "                    combined_listConcepts += value\n",
    "                mostCommonConceptsdict = checkConcepts(combined_listConcepts)\n",
    "                concept_list = list(mostCommonConceptsdict.keys()) #list of top 5 most common concepts for person\n",
    "                #print(concept_list)\n",
    "                #print(merged_dict)\n",
    "                startingWorks = len(list(merged_dict.keys()))\n",
    "                for key in list(merged_dict.keys()):\n",
    "                    checker = False\n",
    "                    for concept in concept_list:\n",
    "                        if concept in merged_dict[key]:\n",
    "                            checker = True\n",
    "                        else:\n",
    "                            pass\n",
    "                    if checker == False:\n",
    "                        del merged_dict[key] \n",
    "                        count +=1\n",
    "                    finalDict[name] = merged_dict\n",
    "                finalWorks = len(list(merged_dict.keys()))\n",
    "                worksRemoved = startingWorks-finalWorks\n",
    "                writer.writerow([name,startingWorks,concept_list, finalWorks, worksRemoved])\n",
    "            #print(f'{count} amount of workIds did not have any of the top 3 most common author concepts in their work concepts')\n",
    "        return finalDict\n",
    "    except Exception as e: \n",
    "        print(\"An exception occurred:\", e)\n",
    "\n",
    "cleanFinalOverview(name_search, \"OpenAlexFilteringWorks.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def completeSearch(title):\n",
    "   listofNames = findAANames()\n",
    "   cleanFinalOverview(listofNames[:15], title)\n",
    "   print(\"ALL DONE!!!!!!!!!\")\n",
    "\n",
    "completeSearch(\"OpenAlexFilteringWorks15.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================BELOW IS TESTING WITH DIFFERENT SYNONYM GENERATORS GIVEN A WORD INPUT========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordhoard import Synonyms\n",
    "\n",
    "def testofTwoLists(names):\n",
    "    dictNames = {}\n",
    "\n",
    "    lst = ['word', 'another', 'word', 'and', 'yet', 'another']\n",
    "    search = ['word', 'and', 'but']\n",
    "    [(w, lst.count(w)) for w in set(lst) if w in search]\n",
    "    [('and', 1), ('word', 2)]\n",
    "\n",
    "    synonym = Synonyms(search_string='medicine')\n",
    "    synonym_results = synonym.find_synonyms()\n",
    "    print(synonym_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load(\"glove-twitter-25\")  # load glove vectors\n",
    "model.most_similar(\"Pediatrics\")  # show words that similar to word 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "  \n",
    "nlp = spacy.load('en_core_web_md')\n",
    "  \n",
    "print(\"Enter two space-separated words\")\n",
    "words = \"pediatrics internal-medicine\"\n",
    "  \n",
    "tokens = nlp(words)\n",
    "  \n",
    "for token in tokens:\n",
    "    # Printing the following attributes of each token.\n",
    "    # text: the word string, has_vector: if it contains\n",
    "    # a vector representation in the model, \n",
    "    # vector_norm: the algebraic norm of the vector,\n",
    "    # is_oov: if the word is out of vocabulary.\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)\n",
    "  \n",
    "token1, token2 = tokens[0], tokens[1]\n",
    "  \n",
    "print(\"Similarity:\", token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordhoard import Synonyms\n",
    "\n",
    "synonym = Synonyms(search_string='Pediatrics')\n",
    "synonym_results = synonym.find_synonyms()\n",
    "print(synonym_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d45d1d5d7130e70f7d881eb43a5e36037fb3ae76395f85a82e56faef1ab333ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
