{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAlex Merging Author ID's\n",
    "Goal: reduce number of author ids per first and last name search and make sure we merge ids to the right people\n",
    "Last updated: 4/05/2023\n",
    "Author: Ashley You"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "William Pao successfully found\n",
      "Ashley You not found\n",
      "Kjersti Aagaard successfully found\n"
     ]
    }
   ],
   "source": [
    "import openalexapi\n",
    "import requests, json\n",
    "import pandas as pd\n",
    "import ast\n",
    "import math\n",
    "import heapq\n",
    "import itertools\n",
    "import csv\n",
    "import os\n",
    "\n",
    "name_search = [\"william pao\",\n",
    "    \"frederick suchy\",\n",
    "    \"malcolm cox\",\n",
    "    \"nancy cooke\",\n",
    "    \"allan sniderman\",\n",
    "    \"vincent dennis\",\n",
    "    \"alessandra pernis\",\n",
    "    \"john minna\",\n",
    "    \"dennis bier\",\n",
    "    \"roger pomerantz\"]\n",
    "\n",
    "\n",
    "#Getting all author ids through search of first and last name:\n",
    "base_url = 'https://api.openalex.org/'\n",
    "def get_authorIDs(name):\n",
    "    listofIDs = []\n",
    "    page = 1\n",
    "    full_query= f'https://api.openalex.org/authors?search={name}&page={page}'\n",
    "    response = requests.get(full_query)\n",
    "    visualize_data = response.json()\n",
    "    num_pages = math.ceil(visualize_data['meta']['count']/25)\n",
    "    \n",
    "    while page <= num_pages:\n",
    "        full_query= f'https://api.openalex.org/authors?search={name}&page={page}'\n",
    "        response = requests.get(full_query)\n",
    "        visualize_data = response.json()\n",
    "        for result in visualize_data['results']:\n",
    "            openalex_id = result['id'].replace(\"https://openalex.org/\", \"\")\n",
    "            listofIDs.append(openalex_id)\n",
    "            \n",
    "            #for concepts in result['x_concepts']:\n",
    "                #if concepts['display_name'] == 'Medicine':\n",
    "        page += 1 \n",
    "\n",
    "    print(f'There are {len(listofIDs)} author ids for {name}')\n",
    "    return listofIDs\n",
    "\n",
    "#get_authorIDs(\"William pao\")\n",
    "\n",
    "\n",
    "#Finds all work_ids with given authorId\n",
    "def work_id(givenAuthorID):\n",
    "        page = 'page={}'\n",
    "        filtered_works_url = f'https://api.openalex.org/works?filter=author.id:{givenAuthorID}&{page}'\n",
    "        page = 1\n",
    "        has_more_pages = True\n",
    "        fewer_than_10000_results = True\n",
    "        all_worksID = []\n",
    "\n",
    "        # loop through pages\n",
    "        while has_more_pages and fewer_than_10000_results:\n",
    "\n",
    "            # set page value and request page from OpenAlex\n",
    "            url = filtered_works_url.format(page)\n",
    "            page_with_results = requests.get(url).json()\n",
    "\n",
    "            # loop through partial list of results\n",
    "            results = page_with_results['results']\n",
    "            for i,work in enumerate(results):\n",
    "                openalex_id = work['id'].replace(\"https://openalex.org/\", \"\")\n",
    "                all_worksID.append(openalex_id)\n",
    "            # next page\n",
    "            page += 1\n",
    "\n",
    "            # end loop when either there are no more results on the requested page \n",
    "            # or the next request would exceed 15 results\n",
    "            per_page = page_with_results['meta']['per_page']\n",
    "            has_more_pages = len(results) == per_page\n",
    "            fewer_than_10000_results = per_page * page <= 10000\n",
    "        print(f'There are {len(all_worksID)} works for {givenAuthorID}')\n",
    "        return (all_worksID)\n",
    "#work_id('A2250212419')\n",
    "\n",
    "#Tests if finding asci names and concepts works under a small scale\n",
    "#----------------------TESTER----------------- FOR THE findAAConcepts FUNCTION AFTER\n",
    "def findIndvConcepts(names):\n",
    "    authorsConcepts = {}\n",
    "    dir = os.path.dirname(os.path.realpath(\"Open_AlexMerging.ipynb\")).replace(\"open_alex_data\", \"asci_aap_data\")\n",
    "    os.chdir(dir)\n",
    "    with open(r\"asci_aap_dataJSONUpdated.json\") as fileJson:\n",
    "        data = json.load(fileJson)\n",
    "        allData = data[\"people\"]\n",
    "\n",
    "    for name in names:\n",
    "        for indv in allData:\n",
    "            first = indv[\"first_name\"].lower()\n",
    "            last = indv[\"last_name\"].lower()\n",
    "            if (first+\" \"+last)== name.lower():                                               \n",
    "                authorsConcepts[name] = (ast.literal_eval(indv[\"original specialization\"]))\n",
    "            \n",
    "        if (name in authorsConcepts):\n",
    "            print(f'{name} successfully found')\n",
    "        else:\n",
    "            authorsConcepts[name] = []\n",
    "            print(f'{name} not found') \n",
    "\n",
    "    return authorsConcepts\n",
    "findIndvConcepts([\"William Pao\",\n",
    "                  \"Ashley You\",\n",
    "                  \"Kjersti Aagaard\"])\n",
    "                #   \"E. Abel\",\n",
    "                #   \"Janis Abkowitz\"])\n",
    "\n",
    "\n",
    "#Goes through asci/aap data and gets name and concepts\n",
    "def findAAConcepts():\n",
    "    authorsConcepts = {}\n",
    "    dir = os.path.dirname(os.path.realpath(\"Open_AlexMerging.ipynb\")).replace(\"open_alex_data\", \"asci_aap_data\")\n",
    "    os.chdir(dir)\n",
    "    with open(r\"asci_aap_dataJSONUpdated.json\") as fileJson:        \n",
    "        data = json.load(fileJson)\n",
    "        allData = data[\"people\"]\n",
    "        print(f'There are {len(allData)} amount of people in ASCI/AAP json file')\n",
    "\n",
    "    for indv in allData:\n",
    "        first = indv[\"first_name\"].lower()\n",
    "        last = indv[\"last_name\"].lower()\n",
    "        name = first+\" \"+last\n",
    "        if len(indv[\"original specialization\"])!= 2:\n",
    "            authorsConcepts[name] = (ast.literal_eval(indv[\"original specialization\"]))\n",
    "    #new_dict = {key: value for key, value in authorsConcepts.items() if value}\n",
    "    print(f'There are {len(authorsConcepts)} amount of people with specialites listed in ASCI/AAP json file')\n",
    "    return authorsConcepts  \n",
    "#findAAConcepts()   \n",
    "\n",
    "def findAANames():\n",
    "    authors = []\n",
    "    dir = os.path.dirname(os.path.realpath(\"Open_AlexMerging.ipynb\")).replace(\"open_alex_data\", \"asci_aap_data\")\n",
    "    os.chdir(dir)\n",
    "    with open(r\"asci_aap_dataJSONUpdated.json\") as fileJson:        \n",
    "        data = json.load(fileJson)\n",
    "        allData = data[\"people\"]\n",
    "        print(f'There are {len(allData)} amount of people in ASCI/AAP json file')\n",
    "\n",
    "    for indv in allData:\n",
    "        first = indv[\"first_name\"].lower()\n",
    "        last = indv[\"last_name\"].lower()\n",
    "        name = first+\" \"+last\n",
    "        authors.append(name)\n",
    "    return authors  \n",
    "#findAANames()\n",
    "\n",
    "#finished\n",
    "#finds author concepts given list of names\n",
    "#filter through medicine \n",
    "def authorConcepts(people):\n",
    "    authors = {}\n",
    "    for name in people:\n",
    "        totalConcepts = []\n",
    "        authorIds = get_authorIDs(name)\n",
    "        for id in authorIds:\n",
    "            authorTopics= {}\n",
    "            tempConcepts = []\n",
    "            full_query= f'https://api.openalex.org/authors/{id}'\n",
    "            response = requests.get(full_query)\n",
    "            visualize_data = response.json()\n",
    "            for concepts in visualize_data[\"x_concepts\"]:\n",
    "                if (float(concepts['score']) >= 90.0 and float(concepts['level']) >= 1) or concepts['display_name']== \"Medicine\":\n",
    "                    tempConcepts.append(concepts['display_name'])\n",
    "            authorTopics[id]= tempConcepts\n",
    "            totalConcepts.append(authorTopics)\n",
    "        authors[name]= totalConcepts\n",
    "    return authors \n",
    "#authorConcepts(['Kjersti Aagaard'])\n",
    "\n",
    "\n",
    "#finds all work details given work id\n",
    "def findWork(workId):\n",
    "    fullquery = base_url+'works/'+workId\n",
    "    response = requests.get(fullquery)\n",
    "    visualize_data = response.json()\n",
    "    visualize_data.pop(\"abstract_inverted_index\")\n",
    "    visualize_data.pop(\"related_works\")\n",
    "    visualize_data.pop(\"ngrams_url\")\n",
    "    #clean the unicode\n",
    "    #visualize_data[\"\"]\n",
    "    return visualize_data\n",
    "#findWork('W2139236349')\n",
    "\n",
    "\n",
    "#finds work concepts given work link\n",
    "def workConcepts(workId):\n",
    "    totalWorkConcepts = []\n",
    "    allinfo = findWork(workId)\n",
    "    for concept in allinfo['concepts']:\n",
    "        if float(concept['score']) >= 0.3 and float(concept['level']) >= 2 or concept['display_name']== \"Medicine\" :\n",
    "            totalWorkConcepts.append(concept['display_name'])\n",
    "    return totalWorkConcepts\n",
    "workConcepts(\"W2005052157\")\n",
    "\n",
    "\n",
    "\n",
    "#checks which concepts occur the most often in a work\n",
    "def checkConcepts(conceptlist):\n",
    "    count_dict = {}\n",
    "    temp_dict = {}\n",
    "    for element in conceptlist:\n",
    "        if element in count_dict:\n",
    "            count_dict[element] += 1\n",
    "        else:\n",
    "            count_dict[element] = 1\n",
    "\n",
    "    for element, count in count_dict.items():\n",
    "        if element!='Medicine':\n",
    "            temp_dict[element] = count\n",
    "    # Find the three largest values\n",
    "    largest_values = heapq.nlargest(3, temp_dict.values())\n",
    "\n",
    "    # Find the keys corresponding to the largest values\n",
    "    largest_keys = []\n",
    "    for key, value in temp_dict.items():\n",
    "        if value in largest_values:\n",
    "            largest_keys.append(key)\n",
    "\n",
    "    # Print the largest values and their keys\n",
    "    final_dict = {}\n",
    "    for i in range(len(largest_values)):\n",
    "        #print(\"{}. {} has a value of {}\".format(i+1, largest_keys[i], largest_values[i]))\n",
    "        final_dict[largest_keys[i]] = largest_values[i]\n",
    "        \n",
    "    return final_dict\n",
    "\n",
    "testList= ['Medicine',\n",
    "  'Medicine', \n",
    "  'Medicine',\n",
    "  'Eosinophilic esophagitis',\n",
    "  'Budesonide',\n",
    "  'Internal medicine',\n",
    "  'Heartburn',\n",
    "  'Eosinophilia',\n",
    "  'Gastroenterology',\n",
    "  'Nausea',\n",
    "  'Vomiting',\n",
    "  'Corticosteroid',\n",
    "  'Adverse effect',\n",
    "  'Esophagitis',\n",
    "  'Eosinophilic esophagitis',\n",
    "  'Internal Medicine',\n",
    "  'Budesonide']\n",
    "#checkConcepts(testList)\n",
    "\n",
    "def findWorkConcepts(names): #keys(1. name, 2. authorId 3. workId)\n",
    "    #searches to get author ids\n",
    "    finalDict = {}\n",
    "    count = 0\n",
    "    for name in names:\n",
    "        authorIDs = get_authorIDs(name)\n",
    "        listofAuthors = []\n",
    "        for id in authorIDs:\n",
    "            tempDict = {}\n",
    "            tempWorkList = []\n",
    "            workIds = work_id(id)\n",
    "            for wID in workIds:\n",
    "               if 'Medicine' in workConcepts(wID)[wID]: #preliminary filter\n",
    "                tempWorkList.append(workConcepts(wID))\n",
    "               else: \n",
    "                   count +=1\n",
    "            tempDict[id] = tempWorkList\n",
    "            listofAuthors.append(tempDict)\n",
    "        finalDict[name] = listofAuthors\n",
    "    print(f'{count} amount of workIds did not have Medicine in their concepts')\n",
    "    return finalDict\n",
    "                \n",
    "    #searches to get work ids\n",
    "    #access work ids\n",
    "    #access concepts in work id\n",
    "    #loops through concepts in work id and saves it \n",
    "#findWorkConcepts([\"Seema Aceves\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following below merges author ids based on their work concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Goal: isolates author id and all of its concepts from the works, testing with 'John Adamson'\n",
    "#Returns: dictionary of ids as keys and list of concepts as value from all of their works with not duplicates \n",
    "def getConcepts(name):\n",
    "    finalDict = {}\n",
    "    allaIDs = get_authorIDs(name)\n",
    "    for aID in allaIDs:\n",
    "        allConcepts = []\n",
    "        listwIDs = work_id(aID)\n",
    "        for wId in listwIDs:\n",
    "            merged_list = list(set(allConcepts) | set(workConcepts(wId))) #merges lists together without any duplicates\n",
    "            allConcepts = (merged_list)\n",
    "        finalDict[aID] = allConcepts\n",
    "        if 'Medicine' not in finalDict[aID]:\n",
    "            del(finalDict[aID])\n",
    "    return finalDict\n",
    "#getConcepts('John Adamson')\n",
    "\n",
    "#Goal: set standard for how much concepts must overlap to be the same author\n",
    "#Return: dictionary of ids as keys and list of ids as values that have more than 4 concepts that they share with the key\n",
    "def identifyIds(name):\n",
    "    try:\n",
    "        totalDict = {}\n",
    "        dictConcepts = getConcepts(name)\n",
    "        allaIDs = list(dictConcepts.keys())\n",
    "\n",
    "        for aID in allaIDs:\n",
    "            authors_similar = []\n",
    "            currentKey = aID\n",
    "            indvConcepts1 = dictConcepts[currentKey]\n",
    "            for key, value in dictConcepts.items():\n",
    "                if currentKey!=key:\n",
    "                    overlap = [x for x in indvConcepts1 if x in value] #finds concepts that are overlapped\n",
    "                    if len(overlap) > 4:\n",
    "                        authors_similar.append(key)\n",
    "            totalDict[aID]= authors_similar\n",
    "            \n",
    "        with open(\"authorIds_dict.json\", \"w\") as json_file:\n",
    "            json.dump(totalDict, json_file)\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "                print(\"---------Error-------------\")\n",
    "    return totalDict\n",
    "\n",
    "#identifyIds('John Adamson') \n",
    "\n",
    "#Goal: merge author ids\n",
    "def mergeIds(name):\n",
    "    inputDict = identifyIds(name)\n",
    "    removedKeys = {}\n",
    "\n",
    "    for key, value in inputDict.items():\n",
    "        tempList = value\n",
    "        tempList.append(key)\n",
    "        for other_key, other_value in inputDict.items():\n",
    "            if key != other_key and (len(value) > 0) and (len(other_value) > 0): \n",
    "                if all(elem in tempList for elem in other_value): #other_value is all in tempList\n",
    "                    removedKeys[other_key] = other_value\n",
    "        tempList.remove(key)\n",
    "\n",
    "    for indv in list(removedKeys.keys()):\n",
    "        del(inputDict[indv])\n",
    "        \n",
    "    print(f'{len(list(removedKeys.keys()))} number of author ids were merged into another id')\n",
    "    print(f'There are {len(list(inputDict.keys()))} author Ids left for {name}')\n",
    "\n",
    "    return inputDict\n",
    "    \n",
    "#mergeIds('John Adamson')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following below is a merging of author ids using instituions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Goal: finds authors institution affiliations for each authorId\n",
    "#Return: dictionary with person name and list of authorids with affiliation \n",
    "def authorInstitutions(name):\n",
    "    totalInstitutions = {}\n",
    "    authorIds = get_authorIDs(name)\n",
    "    for id in authorIds:\n",
    "        full_query= f'https://api.openalex.org/authors/{id}'\n",
    "        response = requests.get(full_query)\n",
    "        visualize_data = response.json()\n",
    "        authorDict = visualize_data[\"last_known_institution\"]\n",
    "        if authorDict == None:\n",
    "            totalInstitutions[id]= None\n",
    "        else:\n",
    "            totalInstitutions[id]= authorDict['display_name']\n",
    "    return totalInstitutions\n",
    "#authorInstitutions('John Adamson')\n",
    "\n",
    "\n",
    "#Goal: Finds authorIds that share affiliation \n",
    "#Return: dictionary with person name and list of lists with authorids that share institution\n",
    "\n",
    "def identifyInstitutions(name):\n",
    "    totalDict = {}\n",
    "    firstDict = authorInstitutions(name)\n",
    "    allaIDs = list(firstDict.keys())\n",
    "    for aID in allaIDs:\n",
    "            authors_similar = []\n",
    "            currentKey = aID\n",
    "            indvInstitution1 = firstDict[currentKey]\n",
    "            if indvInstitution1!= None:\n",
    "                for key, value in firstDict.items():\n",
    "                    if currentKey!=key:\n",
    "                        if value != None:\n",
    "                            if indvInstitution1 == value:\n",
    "                                authors_similar.append(key) \n",
    "            totalDict[aID]= authors_similar\n",
    "    return totalDict\n",
    "#identifyInstitutions('John Adamson')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 35 author ids for John Adamson\n",
      "24 number of author IDs were merged into another ID\n",
      "There are 11 author Ids left for John Adamson\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'A4338988036': ['A4347349823',\n",
       "  'A4345940772',\n",
       "  'A4342530884',\n",
       "  'A4340970251',\n",
       "  'A4335235559',\n",
       "  'A4349866829',\n",
       "  'A4349785327',\n",
       "  'A4331103553',\n",
       "  'A4336431390',\n",
       "  'A4342077751',\n",
       "  'A4350165947',\n",
       "  'A4346282386',\n",
       "  'A4335240014',\n",
       "  'A2406690590',\n",
       "  'A4349232974',\n",
       "  'A4343479288',\n",
       "  'A4350492330',\n",
       "  'A4323480629',\n",
       "  'A4331630488',\n",
       "  'A4337095612',\n",
       "  'A4339183015',\n",
       "  'A4350323753',\n",
       "  'A4352456493'],\n",
       " 'A4334433008': ['A2052798532'],\n",
       " 'A2585988028': [],\n",
       " 'A4304423741': [],\n",
       " 'A3083458673': [],\n",
       " 'A3198642075': [],\n",
       " 'A3200964741': [],\n",
       " 'A4318638943': [],\n",
       " 'A4327582801': [],\n",
       " 'A4346260474': [],\n",
       " 'A4365775785': []}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mergeInstitutions(name):\n",
    "    inputDict = identifyInstitutions(name)\n",
    "    removeKeys = []\n",
    "    checker = []\n",
    "\n",
    "    for key, value in inputDict.items():\n",
    "        tempList = value\n",
    "        tempList.append(key)\n",
    "        for other_key, other_value in inputDict.items():\n",
    "            tempList2 = other_value\n",
    "            tempList2.append(other_key)\n",
    "            if key != other_key and (len(value) > 0) and (len(other_value) > 0): \n",
    "                if all(elem in tempList for elem in other_value) and (tempList not in checker) and (tempList2 not in checker): #other_value is all in tempList\n",
    "                    removeKeys.append(other_key)\n",
    "                    checker.append(tempList)\n",
    "            tempList2.remove(other_key)\n",
    "        tempList.remove(key)\n",
    "\n",
    "    for indv in removeKeys:\n",
    "        del(inputDict[indv])\n",
    "        \n",
    "    print(f'{len(removeKeys)} number of author IDs were merged into another ID')\n",
    "    print(f'There are {len(list(inputDict.keys()))} author Ids left for {name}')\n",
    "\n",
    "    return inputDict\n",
    "\n",
    "mergeInstitutions('John Adamson') #Kjersti Aagaard\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
